{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nCYO6dmGgefe",
        "sPEoabX-hGCh",
        "iLxTNOvI5NHD",
        "XXM2lWhtDYC6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCYO6dmGgefe"
      },
      "source": [
        "# Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er0RD438gRLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be98ac45-066a-4e1e-cd21-73e32f64e308"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfeql_8sgnKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf5f91ab-5472-4ae4-9396-4adf9f282bcd"
      },
      "source": [
        "\"\"\"\n",
        "Change directory to where this file is located\n",
        "\"\"\"\n",
        "%cd '/content/drive/MyDrive/MLDL1/HW3'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MLDL1/HW3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPEoabX-hGCh"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyammZP8hI7P"
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist.data_utils import load_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLxTNOvI5NHD"
      },
      "source": [
        "#Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuQB6W2U5ZE2"
      },
      "source": [
        "def leaky_relu(z):\n",
        "    \"\"\"\n",
        "    Implement the leaky ReLU activation function.\n",
        "    The method takes the input z and returns the output of the function.\n",
        "    \n",
        "    Set the value of alpha for the leaky ReLU funtion to 0.01.\n",
        "    Question (a)\n",
        "\n",
        "    \"\"\"\n",
        "    #### YOUR CODE #####\n",
        "    return np.maximum(0.01 * z, z)\n",
        "    #####################\n",
        "\n",
        "def softmax(X):\n",
        "    \"\"\"\n",
        "    Implement the softmax function.\n",
        "    The method takes the input X and returns the output of the function.\n",
        "\n",
        "    Question (a)\n",
        "\n",
        "    \"\"\"\n",
        "    ##### YOUR CODE #####\n",
        "    X_shifted = X - np.max(X, axis=1, keepdims=True)\n",
        "    exp_X = np.exp(X_shifted)\n",
        "    return exp_X / np.sum(exp_X, axis=1, keepdims=True) # np.exp(x - np.max(x))\n",
        "    #####################\n",
        "\n",
        "def load_batch(X, Y, batch_size, shuffle=True):\n",
        "    \"\"\"\n",
        "    Generates batches with the remainder dropped.\n",
        "\n",
        "    Do NOT modify this function\n",
        "    \"\"\"\n",
        "    if shuffle:\n",
        "        permutation = np.random.permutation(X.shape[0])\n",
        "        X = X[permutation, :]\n",
        "        Y = Y[permutation, :]\n",
        "    num_steps = int(X.shape[0])//batch_size\n",
        "    step = 0\n",
        "    while step<num_steps:\n",
        "        X_batch = X[batch_size*step:batch_size*(step+1)]\n",
        "        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n",
        "        step+=1\n",
        "        yield X_batch, Y_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsU8v_6khR30"
      },
      "source": [
        "#2-Layer Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA5udiGmhRb5"
      },
      "source": [
        "class TwoLayerNN:\n",
        "    \"\"\" a neural network with 2 layers \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, num_hiddens, num_classes):\n",
        "        \"\"\"\n",
        "        Do NOT modify this function.\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_classes = num_classes\n",
        "        self.params = self.initialize_parameters(input_dim, num_hiddens, num_classes)\n",
        "\n",
        "    def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n",
        "        \"\"\"\n",
        "        initializes parameters with He Initialization.\n",
        "\n",
        "        Question (b)\n",
        "        - refer to https://paperswithcode.com/method/he-initialization for He initialization \n",
        "        \n",
        "        Inputs\n",
        "        - input_dim\n",
        "        - num_hiddens\n",
        "        - num_classes\n",
        "        Returns\n",
        "        - params: a dictionary with the initialized parameters.\n",
        "        \"\"\"\n",
        "        params = {}\n",
        "        ##### YOUR CODE #####\n",
        "        params[\"W1\"] = np.random.randn(num_hiddens,input_dim)*np.sqrt(2/input_dim)\n",
        "        params[\"b1\"] = np.zeros((num_hiddens,1))\n",
        "        params[\"W2\"] = np.random.randn(num_classes,num_hiddens) * np.sqrt(2/num_hiddens)  \n",
        "        params[\"b2\"] = np.zeros((num_classes,1))\n",
        "        #####################\n",
        "        \n",
        "        return params\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Define and perform the feed forward step of a two-layer neural network.\n",
        "        Specifically, the network structue is given by\n",
        "\n",
        "          y = softmax(leaky_relu(X W1 + b1) W2 + b2)\n",
        "\n",
        "        where X is the input matrix of shape (N, D), y is the class distribution matrix\n",
        "        of shape (N, C), N is the number of examples (either the entire dataset or\n",
        "        a mini-batch), D is the feature dimensionality, and C is the number of classes.\n",
        "\n",
        "        Question (c)\n",
        "        - ff_dict will be used to run backpropagation in backward method.\n",
        "\n",
        "        Inputs\n",
        "        - X: the input matrix of shape (N, D)\n",
        "\n",
        "        Returns\n",
        "        - y: the output of the model\n",
        "        - ff_dict: a dictionary with all the fully connected units and activations.\n",
        "        \"\"\"\n",
        "        ff_dict = {}\n",
        "        ##### YOUR CODE #####\n",
        "        ff_dict[\"z1\"] = np.dot(X, self.params[\"W1\"].T) + self.params[\"b1\"].T \n",
        "        ff_dict[\"a1\"] = leaky_relu(ff_dict[\"z1\"]) \n",
        "        ff_dict[\"z2\"] = np.dot(ff_dict[\"a1\"], self.params[\"W2\"].T) + self.params[\"b2\"].T \n",
        "        y = softmax(ff_dict[\"z2\"]) \n",
        "        ff_dict[\"y\"] = y\n",
        "        #####################\n",
        "        return y, ff_dict\n",
        "\n",
        "    def backward(self, X, Y, ff_dict):\n",
        "        \"\"\"\n",
        "        Performs backpropagation over the two-layer neural network, and returns\n",
        "        a dictionary of gradients of all model parameters.\n",
        "\n",
        "        Question (d)\n",
        "\n",
        "        Inputs:\n",
        "         - X: the input matrix of shape (B, D), where B is the number of examples\n",
        "              in a mini-batch, D is the feature dimensionality.\n",
        "         - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n",
        "              where B is the number of examples in a mini-batch, C is the number\n",
        "              of classes.\n",
        "         - ff_dict: the dictionary containing all the fully connected units and\n",
        "              activations.\n",
        "\n",
        "        Returns:\n",
        "         - grads: a dictionary containing the gradients of corresponding weights and biases.\n",
        "        \"\"\"\n",
        "        grads = {}\n",
        "        ##### YOUR CODE #####\n",
        "        '''\n",
        "        grads[\"dW1\"] = None\n",
        "        grads[\"db1\"] = None\n",
        "        grads[\"dW2\"] = None\n",
        "        grads[\"db2\"] = None\n",
        "        '''\n",
        "\n",
        "        # Compute gradients of the last layer\n",
        "        dZ2 = (ff_dict[\"y\"] - Y) \n",
        "        grads[\"dW2\"] = np.dot(ff_dict[\"a1\"].T, dZ2).T \n",
        "        grads[\"db2\"] = np.sum(dZ2, axis=0).reshape(-1,1)\n",
        "\n",
        "        # Compute gradients of the hidden layer\n",
        "        dZ1 = np.dot(dZ2, self.params[\"W2\"])*np.where(ff_dict['z1'] >=0, 1, 0.01)\n",
        "        grads[\"dW1\"] = np.dot(dZ1.T, X)\n",
        "        grads[\"db1\"] = np.sum(dZ1, axis=0).reshape(-1,1)\n",
        "\n",
        "        ##############\n",
        "        return grads\n",
        "\n",
        "    def compute_loss(self, Y, Y_hat):\n",
        "        \"\"\"\n",
        "        Computes cross entropy loss.\n",
        "\n",
        "        Do NOT modify this function.\n",
        "\n",
        "        Inputs\n",
        "            Y:\n",
        "            Y_hat:\n",
        "        Returns\n",
        "            loss:\n",
        "        \"\"\"\n",
        "        epsilon = 1e-10\n",
        "        Y_hat = np.clip(Y_hat, epsilon, 1 - epsilon)\n",
        "        loss = -(1/Y.shape[0]) * np.sum(np.multiply(Y, np.log(Y_hat)))\n",
        "        return loss\n",
        "\n",
        "    def train(self, X, Y, X_val, Y_val, lr, n_epochs, batch_size, log_interval=1):\n",
        "        \"\"\"\n",
        "        Runs mini-batch gradient descent.\n",
        "\n",
        "        Do NOT Modify this method.\n",
        "\n",
        "        Inputs\n",
        "        - X\n",
        "        - Y\n",
        "        - X_val\n",
        "        - Y_Val\n",
        "        - lr\n",
        "        - n_epochs\n",
        "        - batch_size\n",
        "        - log_interval\n",
        "        \"\"\"\n",
        "        for epoch in range(n_epochs):\n",
        "            for X_batch, Y_batch in load_batch(X, Y, batch_size):\n",
        "                self.train_step(X_batch, Y_batch, batch_size, lr)\n",
        "            if epoch % log_interval==0:\n",
        "                Y_hat, ff_dict = self.forward(X)\n",
        "                train_loss = self.compute_loss(Y, Y_hat)\n",
        "                train_acc = self.evaluate(Y, Y_hat)\n",
        "                Y_hat, ff_dict = self.forward(X_val)\n",
        "                valid_loss = self.compute_loss(Y_val, Y_hat)\n",
        "                valid_acc = self.evaluate(Y_val, Y_hat)\n",
        "                print('epoch {:02} - train loss/acc: {:.3f} {:.3f}, valid loss/acc: {:.3f} {:.3f}'.\\\n",
        "                      format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
        "\n",
        "    def train_step(self, X_batch, Y_batch, batch_size, lr):\n",
        "        \"\"\"\n",
        "        Updates the parameters using gradient descent.\n",
        "\n",
        "        Do NOT Modify this method.\n",
        "\n",
        "        Inputs\n",
        "        - X_batch\n",
        "        - Y_batch\n",
        "        - batch_size\n",
        "        - lr\n",
        "        \"\"\"\n",
        "        _, ff_dict = self.forward(X_batch)\n",
        "        grads = self.backward(X_batch, Y_batch, ff_dict)\n",
        "        self.params[\"W1\"] -= lr * grads[\"dW1\"]/batch_size\n",
        "        self.params[\"b1\"] -= lr * grads[\"db1\"]/batch_size\n",
        "        self.params[\"W2\"] -= lr * grads[\"dW2\"]/batch_size\n",
        "        self.params[\"b2\"] -= lr * grads[\"db2\"]/batch_size\n",
        "\n",
        "    def evaluate(self, Y, Y_hat):\n",
        "        \"\"\"\n",
        "        Computes classification accuracy.\n",
        "        \n",
        "        Do NOT modify this function\n",
        "\n",
        "        Inputs\n",
        "        - Y: A numpy array of shape (N, C) containing the softmax outputs,\n",
        "             where C is the number of classes.\n",
        "        - Y_hat: A numpy array of shape (N, C) containing the one-hot encoded labels,\n",
        "             where C is the number of classes.\n",
        "\n",
        "        Returns\n",
        "            accuracy: the classification accuracy in float\n",
        "        \"\"\"        \n",
        "        classes_pred = np.argmax(Y_hat, axis=1)\n",
        "        classes_gt = np.argmax(Y, axis=1)\n",
        "        accuracy = float(np.sum(classes_pred==classes_gt)) / Y.shape[0]\n",
        "        return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXM2lWhtDYC6"
      },
      "source": [
        "#Load MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48ooR6YIxYhC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762e8d88-c93e-4d87-d3f9-a07d105aa162"
      },
      "source": [
        "X_train, Y_train, X_test, Y_test = load_data()\n",
        "\n",
        "idxs = np.arange(len(X_train))\n",
        "np.random.shuffle(idxs)\n",
        "split_idx = int(np.ceil(len(idxs)*0.8))\n",
        "X_valid, Y_valid = X_train[idxs[split_idx:]], Y_train[idxs[split_idx:]]\n",
        "X_train, Y_train = X_train[idxs[:split_idx]], Y_train[idxs[:split_idx]]\n",
        "print()\n",
        "print('Set validation data aside')\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Training labels shape: ', Y_train.shape)\n",
        "print('Validation data shape: ', X_valid.shape)\n",
        "print('Validation labels shape: ', Y_valid.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST data loaded:\n",
            "Training data shape: (60000, 784)\n",
            "Training labels shape: (60000, 10)\n",
            "Test data shape: (10000, 784)\n",
            "Test labels shape: (10000, 10)\n",
            "\n",
            "Set validation data aside\n",
            "Training data shape:  (48000, 784)\n",
            "Training labels shape:  (48000, 10)\n",
            "Validation data shape:  (12000, 784)\n",
            "Validation labels shape:  (12000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzw-D4Zr5xoi"
      },
      "source": [
        "#Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlnC_rerHPaN"
      },
      "source": [
        "### \n",
        "# Question (e)\n",
        "# Tune the hyperparameters with validation data, \n",
        "# and print the results by running the lines below.\n",
        "###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTCqVT4S0Tm5"
      },
      "source": [
        "# model instantiation\n",
        "model = TwoLayerNN(input_dim=784, num_hiddens=64, num_classes=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cWb6xg0NxOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9889947-4c16-41e7-f842-5c3822e40b07"
      },
      "source": [
        "# train the model\n",
        "# I have tried multiple sets of combination, and concluded with the below hyperparameters.\n",
        "lr, n_epochs, batch_size = 0.02, 20, 256 # changed the hyperparameters. (lr from 3-> 0.02)\n",
        "model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 00 - train loss/acc: 0.788 0.828, valid loss/acc: 0.790 0.830\n",
            "epoch 01 - train loss/acc: 0.532 0.870, valid loss/acc: 0.537 0.869\n",
            "epoch 02 - train loss/acc: 0.446 0.883, valid loss/acc: 0.450 0.882\n",
            "epoch 03 - train loss/acc: 0.401 0.893, valid loss/acc: 0.405 0.891\n",
            "epoch 04 - train loss/acc: 0.373 0.899, valid loss/acc: 0.377 0.897\n",
            "epoch 05 - train loss/acc: 0.354 0.902, valid loss/acc: 0.359 0.899\n",
            "epoch 06 - train loss/acc: 0.338 0.906, valid loss/acc: 0.343 0.904\n",
            "epoch 07 - train loss/acc: 0.326 0.908, valid loss/acc: 0.331 0.907\n",
            "epoch 08 - train loss/acc: 0.315 0.911, valid loss/acc: 0.321 0.909\n",
            "epoch 09 - train loss/acc: 0.306 0.914, valid loss/acc: 0.313 0.912\n",
            "epoch 10 - train loss/acc: 0.299 0.916, valid loss/acc: 0.306 0.914\n",
            "epoch 11 - train loss/acc: 0.292 0.917, valid loss/acc: 0.299 0.916\n",
            "epoch 12 - train loss/acc: 0.285 0.920, valid loss/acc: 0.293 0.918\n",
            "epoch 13 - train loss/acc: 0.279 0.922, valid loss/acc: 0.287 0.920\n",
            "epoch 14 - train loss/acc: 0.274 0.923, valid loss/acc: 0.282 0.921\n",
            "epoch 15 - train loss/acc: 0.269 0.925, valid loss/acc: 0.278 0.923\n",
            "epoch 16 - train loss/acc: 0.264 0.926, valid loss/acc: 0.273 0.924\n",
            "epoch 17 - train loss/acc: 0.260 0.928, valid loss/acc: 0.270 0.925\n",
            "epoch 18 - train loss/acc: 0.255 0.929, valid loss/acc: 0.265 0.926\n",
            "epoch 19 - train loss/acc: 0.251 0.930, valid loss/acc: 0.261 0.927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpPsAlXU0T_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e21c3eca-466c-48e3-a967-221792c183ac"
      },
      "source": [
        "# evalute the model on test data\n",
        "Y_hat, _ = model.forward(X_test)\n",
        "test_loss = model.compute_loss(Y_test, Y_hat)\n",
        "test_acc = model.evaluate(Y_test, Y_hat)\n",
        "print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final test loss = 0.246, acc = 0.932\n"
          ]
        }
      ]
    }
  ]
}